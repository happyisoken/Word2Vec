{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f60fe78-9e69-44c8-be1e-5701be5167be",
   "metadata": {},
   "source": [
    "#### Introduction to Word Embedding and Word2Vec\n",
    "\n",
    "##### How does Word2Vec work?\n",
    "\n",
    "Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): \n",
    "- Skip Gram \n",
    "- Common Bag Of Words (CBOW)\n",
    "\n",
    "##### CBOW Model:\n",
    "This method takes the context of each word as the input and tries to  redict the word corresponding to the context.\n",
    "\n",
    "##### Skip-Gram model:\n",
    "This looks like the multiple-context CBOW model just got flipped. To some extent that is true. We input the target word into the network. \n",
    "\n",
    "In both cases, the network uses back-propagation to learn.\n",
    "\n",
    "##### Who wins?\n",
    "Both have their own advantages and disadvantages. According to Mikolov,\n",
    "- Skip Gram works well with small amounts of data and is found to represent rare words well.\n",
    "- On the other hand, CBOW is faster and has better representations for more frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217f10d-e430-42bc-8702-c6538a236a6a",
   "metadata": {},
   "source": [
    "#### Word2Vec Detailed Explanation and Train your custom Word2Vec Model using genism in Python - #NLProc tutorial.\n",
    "\n",
    "##### Motivation\n",
    "- Audio: Audio spectrogram is Dense\n",
    "- Image: Image pixels are Dense\n",
    "- Text: Word, content, or document vectors are Sparse\n",
    "\n",
    "Word2Vec is a way of efficient estimation of word representations in Vector space.\n",
    "\n",
    "##### Word2Vec Hyperparameters\n",
    "\n",
    "1. Number of negative samples\n",
    " - The original paper prescribes 5-20 as good number of negative samples.\n",
    " - It also states that 2-5 seems to be enough when you have a large enough dataset.\n",
    "2. Window size\n",
    " - Smaller window sizes(2-15) lead to embeddings where high similarity scores between two embeddings indicates that the words are interchangeable.\n",
    " - Larger window sizes(15-50, or even more) lead to embeddings wher similarity is more indicative of relatedness of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56848815-febc-4f42-ad78-636e4153b6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\user\\anaconda3\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.9.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ea4393-7417-42ce-afd2-9cd43d390095",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11344\\1870825812.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec-google-news-300'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\corpora\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[1;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b00ff2a-2545-437a-9c56-358c7efeccca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11344\\2037890106.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wv' is not defined"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c505a-a25c-461d-ae2a-996b747ad7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = wv['king']\n",
    "print(vec_king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3a7a8-dc16-46e1-ab75-db3a279065c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'), # a minivan is a kind of car\n",
    "    ('car', 'bicycle'), # still a wheeled vehicle\n",
    "    ('car', 'airplane'), # no wheels but still a vehicle\n",
    "    ('car', 'cereal'),   # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f8c5a-f2a6-4212-a9d8-518933955266",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['car', 'minivan'], topn = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd611b-0ced-4f58-afaf-2058a6a22a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6da5a6d9-0445-4116-9db6-024796b9144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d48e4-b05e-469a-b517-57a7d78945ce",
   "metadata": {},
   "source": [
    "The dataset is from Amazon Review Data (2018). Here, we look at cell phones and Accessories review dataset from \"Small\" subsets for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7fe2d-b6c4-44e7-9703-30380b3eb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for line in open('C:\\\\AmazonReviewsCellPhones\\\\Cell_Phones_and_Accessories')\n",
    "    data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81a1e7-6230-4908-999a-af1ef4ee3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])\n",
    "df = pd.DataFrame(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4623193-dc8b-4652-847f-71d7c8255423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)\n",
    "df = df.drop(columns = ['reviewerName', 'vote', 'image', 'style'])\n",
    "df1 = df.rename(columns = {'overall': 'rating', 'asin': 'productID'}, inplace = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abce2d-0001-43fe-8fee-c9348f6339a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna(axis = 0, how = 'any', inplace = True)\n",
    "df1.drop_duplicates(subset = ['rating', 'reviewText'], keep = 'first', inplace = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3c6c3-23aa-4190-9393-b81e527be004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text ):\n",
    "    delete_dict = {sp_character: ' ' for sp_character in string.punctuation}\n",
    "    delete_dict[' '] = ' '\n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    #print('cleaned:'+text1)\n",
    "    textArr = text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and (not w.isdigit))\n",
    "    \n",
    "    return text2.lower().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16e7c3-f483-4913-87b9-ba01a4a9e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.sample(n=200000)\n",
    "df2['reviewText']= df2['reviewText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed3142-3616-48ab-9316-a27266768a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df2['reviewText'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb28830-4eb4-4c6c-a730-b74fd7ef39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[1])\n",
    "print(sentences[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3aea9b-8159-4fb8-ba74-ad5dd3156627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b54558-be12-4a62-b6de-ca9fa842f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# init callback class\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def _init_(self):\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        \n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        elif self.epoch % 100 == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss-self.loss))\n",
    "            \n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce3119-99d9-45ca-8e31-8dd45b2a9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init word2vec class\n",
    "w2v_model = Word2Vec(size = 300,\n",
    "                     window = 15,\n",
    "                     min_count = 2,\n",
    "                     workers = 20c,\n",
    "                     sg = 1,\n",
    "                     negative = 5,\n",
    "                     sample = 1e-5)\n",
    "# build vocab\n",
    "\n",
    "w2v_model.build_vocab(sentences)\n",
    "\n",
    "# train the w2v model\n",
    "start = time.time()\n",
    "w2v_model.train(sentences,\n",
    "                total_examples=w2v_model.corpus_count,\n",
    "                epochs=1001,\n",
    "                report_delay=1,\n",
    "                compute_loss = True, # set compute_loss = True\n",
    "                callbacks=[callback()]) # add the callback class\n",
    "end = time.time()\n",
    "\n",
    "print(\"elapsedtime in seconds :\"+ str(end - start))\n",
    "# save the word2vec model\n",
    "w2v_model.save('C:\\\\AmazonReviewsCellPhones\\\\word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a6a3c-81d8-40fe-a905-5d8a19ba9f22",
   "metadata": {},
   "source": [
    "Let us reload our word2vec model and perform operations using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf54289-8f39-4ede-a772-7eb2439710c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_w2v_model = Word2Vec.load('C:\\\\AmazonReviewsCellPhones\\\\word2vec.model')\n",
    "words = list(reloaded_w2v_model.wv.vocab)\n",
    "print('Vocab size: '+str(len(words)))\n",
    "w1 = 'cancellation'\n",
    "print(\"Top 3 words similar to cancellation:\",\\\n",
    "      reloaded_w2v_model.wv.most_similar(positive = w1, topn =3))\n",
    "w1 = 'poor'\n",
    "print(\"Top 3 words similar to poor:\",\\\n",
    "      reloaded_w2v_model.wv.most_similar(positive = w1, topn =3)\n",
    "print(\"Similarity between earphones and headphones:\"+\\\n",
    "      str(reloaded_w2v_model.wv.similarity(w1=\"earphones\",w2=\"headphones\")))\n",
    "print(\"Similarity between charger and charge:\"+\\\n",
    "      str(reloaded_w2v_model.wv.similarity(w1=\"charger\",w2=\"charge\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee3272-c812-48bf-bad8-f07af27cb5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE              # final reduction\n",
    "import numpy as np                             # array handling\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimesions (2D, 3D, etc)\n",
    "    \n",
    "    vectors = [] # positions in vector space\n",
    "    labels = []  # keep track of words to label our data again later\n",
    "    for word in model.wv.vocab:\n",
    "        vectors.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "        \n",
    "        # covert both lists into numpy vectors for reduction\n",
    "        vectors = np.asarray(vectors)\n",
    "        #labels = np.asarray(labels)\n",
    "        \n",
    "        # reduce using t-SNE\n",
    "        vectors = np.asarray(vectors)\n",
    "        tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "        vectors = tsne.fit_transform(vectors)\n",
    "        \n",
    "        x_vals = [v[0] for v in vectors]\n",
    "        y_vals = [v[1] for v in vectors]\n",
    "        return x_vals, y_vals, labels\n",
    "    \n",
    "x_vals, y_vals, labels = reduce_dimensions(reloaded_w2v_model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7959f-a73f-4f6b-9988-91c0d9bd1634",
   "metadata": {},
   "source": [
    "Les us visualize our word2vec model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f170a-68af-43a1-9281-e47b0644a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_matpltlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "    \n",
    "    # Label randomly subsampled 25 data points\n",
    "    \n",
    "    indices = list(range(len(labels)))\n",
    "    #selected_indices = random.sample(indices, 25)\n",
    "    selected_indices=[]\n",
    "    index = labels.index(\"cell\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"phone\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"noise\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"cancellation\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"charger\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"charge\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"poor\")\n",
    "    selected_indices.append(index)\n",
    "    index = labels.index(\"bad\")\n",
    "    selected_indices.append(index)\n",
    "    \n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "        \n",
    "plot_function = plot_with_matplotlib\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec3352-b229-4993-89d4-81c66e7e95b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
